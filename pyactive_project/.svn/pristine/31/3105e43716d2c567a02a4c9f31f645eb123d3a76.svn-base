======================================
			  PyActive
======================================

## Description
PyActive -Active Object Middleware- is a novel distributed object framework inspired in
Erlang’s actor model. From Erlang it borrows the use of lightweight processes, 
asynchronous message-passing concurrency, and supervisor fault-tolerant facilities.
The main features of PyActive are:

* We follow a pure object oriented approach for method invocation. 
  Other actor frameworks use special notations (!) for sending messages 
  and pattern matching or conditionals to receive them. Instead of that, PyActive 
  middleware transparently maps messages to methods and thus achieving better 
  code expressiveness.

* Furthermore, PyActive hides continuations and microthreads in RPC stubs to 
  elegantly mask synchronous calls as asynchronous messages.

* The resulting code is easier to follow since it respects data-flow semantics.

* Finally, we offer advanced abstractions like remote reference passing, and
  exception handling to easethe implementation of distributed algorithms. 

PyActive has been implemented in Python over the Stackless Python project 
(http://www.stackless.com/) and over Python threads using library threading. 
In the near future we’re going to implement for others modules. We validated the
performance and expressiveness of PyActive to code distributed algorithms.
We demonstrated PyActive can considerably simplify development and simulation
of distributed algorithms.

## Basic annotations

* **async**: It’s used to indicate the method can receive asynchronous remote calls.

* **sync**: It’s used to indicate the method can receive synchronous remote calls.
  So it’s necessary to return something.

* **parallel**: It’s used to  complement the asynchronous methods that need to call some
  synchronous methods. Because if we don't put this annotation, this method will lock
  the principal process, but if we put this annotation this method will execute in another
  thread, or microthread, and it won’t look the principal process. Indeed it’ll serve more petitions.

* **ref**: It’s used to remark the method can receive or return some PyActive object.
  So this annotation guarantees the pass-by-reference 

## Basic Functions
**start_controller**: It's used to choose the module. At this moment, we can choose 
  between 'pyactive_thread' and 'tasklet'. Note that this decision can change the 
  Python version that you need. For example the 'tasklet' module needs Stackless Python. 
  
  **launch**: It's used to throw the main function which initializates the program. Once this function ends, the program will die.

** serve_forever**. It’s used like launch function but once the function ends, the program continues.

## how to use PyActive?

In this section we explain all you need to use this middleware. It's easy!

**Requirements**
* If you only use the threads module, you only need Python 2.7

* If you need use the stackless version, you need Python 2.7 with 
  Stackless Python

You can download Python in: http://www.python.org/download/ 

Once you have installed python, the next step is to install Stackless python.
You can download Stackless python at: http://www.stackless.com/

**Obtaining and using PyActive**
PyActive can be found on our repository (subversion, read only access) : 
http://www.svnPedro.com

PyActive contains some examples and tests. You can run the following tests:

	$> cd/pyactive
	$> python ./test/Hello_World/test_async.py
	$> python ./test/registry/first.py

Choose the module using the function: 'start_controller'.  Nowadays, 
you can put either the parameter 'tasklet' or 'pyactive_thread' to choose the module.
Note that you choose the tasklet module, you need the Stacklees Python. 



## Perspectives uses and future work

* **Simulation and implementation of distributed algorithms**: PyActive can 
  considerably simplify the development of distributed algorithms. It is 
  possible to simulate algorithms in a single machine before they are 
  deployed in an experimentation testbed. We implemented Chord in the past 
  for an event-based traditional p2p simulator (PlanerSim) and the code is 
  complex to understand and follow. On the other hand, our implementation 
  using PyActive is more succinct, and quit similar to the original algorithms 
  proposed in the Chord paper. The main reason is that PyActive clearly separates 
  communication code from algorithm code inside methods. We plan to use PyActive 
  in our distributed systems course and in our peer-to-peer and networking courses.

* **Web middleware**: PyActive has a big potential to ease the development of REST 
  and Web RPC platforms. Web asynchronous networking libraries that use green 
  threads. We are even considering to create a novel version of PyActive on top of
  one these libraries. In particular, we plan to rewrite some server code of the
  OpenStack Swift is based on WSGI python servers that already use green threads
  to improve the performance. But the current code is tangling communication, 
  marshalling and distributed storage algorithms. PyActive can decouple these layers
  and make the code performance and easy to understand ans modify

* **Multi-core programming**: One of the promises of message passing concurrency 
  is the future of multi-core concurrency programming. Erlang offers truly 
  parallelism over different cores with Symmetric Multi-Processing(SMP). They mainly 
  added multi-threading support to the Erlang VM, so that different lightweight 
  process schedulers can live inside their native threads. In our case, stackless 
  also permits to have different microthreads living inside their own thread or 
  process. But python threads do not benefit from multi-core programming due to 
  the GIL (Global Interpreter lock). Instead of that, we could support multi-core 
  programming using the python multiprocessing library.

* **Distributed continuations**: We outline an important future work with the 
  combination of RPCs and distributed continuations in PyActive. Stackless python 
  permits to serialize microthreads in their current frames. It is thus feasible
  to create novel call abstractions supporting distributed continuations between
  different hosts.


